{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from BERT_WordEmbeddingsPipeline import embeddingsPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "SEED = 25\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hpl.csv')\n",
    "data = data[:100]\n",
    "\n",
    "embedding_length = 768\n",
    "max_words = 100\n",
    "\n",
    "zero_embedding = [0 for i in range(embedding_length)]\n",
    "\n",
    "emb = []\n",
    "for i in data['tokenized_sents']:\n",
    "    e = embeddingsPipeline(i)\n",
    "    while(len(e) < max_words):\n",
    "        e.append(zero_embedding)\n",
    "    e = e[:max_words]\n",
    "    emb.append(e)\n",
    "data['embeddings'] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = embedding_length\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = embedding_length\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.Tensor(data['embeddings'])\n",
    "dataSet = torch.utils.data.TensorDataset(emb)\n",
    "dataLoader = torch.utils.data.DataLoader(dataSet, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoder import Encoder\n",
    "from Decoder import Decoder\n",
    "from Generator import Generator\n",
    "from Discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (lstm): LSTM(768, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm): LSTM(512, 768, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (fc1): Linear(in_features=1536, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (lstm): LSTM(768, 1, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS)\n",
    "dec = Decoder(HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n",
    "gen = Generator(enc, dec, max_words, HIDDEN_SIZE, embedding_length)\n",
    "print(gen)\n",
    "\n",
    "dis = Discriminator(INPUT_SIZE, NUM_LAYERS, max_words)\n",
    "print(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataLoader, gen, dis, num_epochs, max_words, batch_size, embedding_length):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gen.to(device)\n",
    "    dis.to(device)\n",
    "    \n",
    "    dis.train()\n",
    "    gen.train()\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    d_optimizer = torch.optim.Adam(dis.parameters(), lr=0.0002)\n",
    "    g_optimizer = torch.optim.Adadelta(gen.parameters(), lr=0.0004)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        real_labels = torch.ones(1, batch_size).to(device)\n",
    "        fake_labels = torch.zeros(1, batch_size).to(device)\n",
    "        \n",
    "        #---------------------------------------------------#\n",
    "        # Training Discriminator\n",
    "        #---------------------------------------------------#\n",
    "        total_discriminator_loss = 0\n",
    "        iterator = iter(dataLoader)\n",
    "        \n",
    "        for i in range(len(dataLoader)):\n",
    "            batch_data = next(iterator) \n",
    "            # batch_data is a one-element list containing a single batch in the form of Tensor.\n",
    "            batch = batch_data[0]\n",
    "            \n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            dis_output = torch.zeros(batch_size)\n",
    "            for i, embedding in enumerate(batch):\n",
    "                dis_output[i] = dis(embedding.view(1, max_words, embedding_length))\n",
    "            \n",
    "            dis_output = dis_output.to(device)\n",
    "            d_loss_real = criterion(dis_output, real_labels)\n",
    "            \n",
    "            fake_batch = gen(batch, 0.5)\n",
    "            fake_batch = fake_batch.to(device)\n",
    "            \n",
    "            dis_output = torch.zeros(batch_size)\n",
    "            for i, embedding in enumerate(fake_batch):\n",
    "                dis_output[i] = dis(embedding.view(1, max_words, embedding_length))\n",
    "                \n",
    "            dis_output = dis_output.to(device)\n",
    "            d_loss_fake = criterion(dis_output, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            total_discriminator_loss += d_loss.item()\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "    \n",
    "        #---------------------------------------------------#\n",
    "        # Training Generator\n",
    "        #---------------------------------------------------#\n",
    "        total_generator_loss = 0\n",
    "        iterator = iter(dataLoader)\n",
    "        \n",
    "        for i in range(len(dataLoader)):\n",
    "            batch_data = next(iterator) \n",
    "            # batch_data is a one-element list containing a single batch in the form of Tensor.\n",
    "            batch = batch_data[0]\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            generated_batch = gen(batch, 0.5)\n",
    "            generated_batch = generated_batch.to(device)\n",
    "            \n",
    "            dis_output = torch.zeros(batch_size)\n",
    "            for i, embedding in enumerate(generated_batch):\n",
    "                dis_output[i] = dis(embedding.view(1, max_words, embedding_length))\n",
    "            \n",
    "            dis_output = dis_output.to(device)\n",
    "            g_loss = criterion(dis_output, real_labels)\n",
    "            total_generator_loss += g_loss.item()\n",
    "            \n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "        finish_time = time.time()\n",
    "        avg_d_loss = total_discriminator_loss/len(dataLoader)\n",
    "        avg_g_loss = total_generator_loss/len(dataLoader)\n",
    "        time_taken = finish_time-start_time\n",
    "        \n",
    "        print(\"Epoch[{:02}/{}]: average_d_Loss: {:.4f}, average_g_Loss: {:.4f}, Time for epoch: {:.4f}\" \n",
    "             .format(epoch+1, num_epochs, avg_d_loss, avg_g_loss, time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[01/10]: average_d_Loss: 1.3871, average_g_Loss: 0.6833, Time for epoch: 85.4287\n",
      "Epoch[02/10]: average_d_Loss: 1.3866, average_g_Loss: 0.6932, Time for epoch: 89.9586\n",
      "Epoch[03/10]: average_d_Loss: 1.3867, average_g_Loss: 0.7018, Time for epoch: 95.5795\n",
      "Epoch[04/10]: average_d_Loss: 1.3845, average_g_Loss: 0.7064, Time for epoch: 103.9183\n",
      "Epoch[05/10]: average_d_Loss: 1.3855, average_g_Loss: 0.7093, Time for epoch: 93.7973\n",
      "Epoch[06/10]: average_d_Loss: 1.3852, average_g_Loss: 0.7104, Time for epoch: 98.3984\n",
      "Epoch[07/10]: average_d_Loss: 1.3851, average_g_Loss: 0.7097, Time for epoch: 102.0737\n",
      "Epoch[08/10]: average_d_Loss: 1.3843, average_g_Loss: 0.7079, Time for epoch: 103.8758\n",
      "Epoch[09/10]: average_d_Loss: 1.3819, average_g_Loss: 0.7070, Time for epoch: 107.3690\n",
      "Epoch[10/10]: average_d_Loss: 1.3823, average_g_Loss: 0.7069, Time for epoch: 103.0192\n"
     ]
    }
   ],
   "source": [
    "train(dataLoader, gen, dis, 10, max_words, BATCH_SIZE, embedding_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
