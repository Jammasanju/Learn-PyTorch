{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-11 16:28:21 - Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "2020-05-11 16:28:21 - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-05-11 16:28:21 - Downloading sentence transformer model from https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/bert-base-nli-mean-tokens.zip and saving it at C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405M/405M [04:42<00:00, 1.43MB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-11 16:33:13 - Load SentenceTransformer from folder: C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "2020-05-11 16:33:13 - loading configuration file C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\config.json\n",
      "2020-05-11 16:33:13 - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-05-11 16:33:13 - loading weights file C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\pytorch_model.bin\n",
      "2020-05-11 16:33:16 - Model name 'C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-05-11 16:33:16 - Didn't find file C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\tokenizer_config.json. We won't load it.\n",
      "2020-05-11 16:33:16 - loading file C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\vocab.txt\n",
      "2020-05-11 16:33:16 - loading file C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\added_tokens.json\n",
      "2020-05-11 16:33:16 - loading file C:\\Users\\chait/.cache\\torch\\sentence_transformers\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\0_BERT\\special_tokens_map.json\n",
      "2020-05-11 16:33:16 - loading file None\n",
      "2020-05-11 16:33:16 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Sentence model (based on BERT) from URL\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Embed a list of sentences\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-0.10409436  0.52747667  1.1797729  ... -0.4338915  -0.6945236\n",
      "  0.5386925 ]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [-0.13118412 -0.1739029   1.1052189  ...  0.02624453 -0.00269856\n",
      "  0.91611093]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [-0.74899274  0.71891785 -1.039457   ...  0.15582602  1.0202513\n",
      "  0.09790424]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The result is a list of sentence embeddings as numpy arrays\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9352)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "setA = ['They were making too much noise']\n",
    "setB = ['They were being too noisy']\n",
    "\n",
    "esetA = model.encode(setA)\n",
    "esetB = model.encode(setB)\n",
    "\n",
    "import torch\n",
    "\n",
    "v1 = torch.from_numpy(esetA[0])\n",
    "v2 = torch.from_numpy(esetB[0])\n",
    "\n",
    "import torch.nn as nn\n",
    "cos = nn.CosineSimilarity(dim=0) \n",
    "print (cos(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2007)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "setA = [\"I haven't done it for years.\"]\n",
    "setB = ['They were being too noisy']\n",
    "\n",
    "esetA = model.encode(setA)\n",
    "esetB = model.encode(setB)\n",
    "v1 = torch.from_numpy(esetA[0])\n",
    "v2 = torch.from_numpy(esetB[0])\n",
    "cos = nn.CosineSimilarity(dim=0) \n",
    "print (cos(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8881)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "setA = [\"I haven't done it for years.\"]\n",
    "setB = [\"It's been years since I did it.\"]\n",
    "\n",
    "esetA = model.encode(setA)\n",
    "esetB = model.encode(setB)\n",
    "v1 = torch.from_numpy(esetA[0])\n",
    "v2 = torch.from_numpy(esetB[0])\n",
    "cos = nn.CosineSimilarity(dim=0) \n",
    "print (cos(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
