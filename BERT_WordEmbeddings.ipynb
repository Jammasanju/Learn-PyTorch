{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# !pip install pytorch-pretrained-bert\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BERT is a pretrained model that expects input data in a specific format, we will need:\n",
    "    -->special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -->tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -->token IDs from BERT’s tokenizer\n",
    "    -->mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -->segment IDs used to distinguish different sentences\n",
    "    -->positional embeddings used to show token position within the sequence\n",
    "\"\"\"\n",
    "\n",
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAfter breaking the text into tokens, \\nwe then have to convert the sentence from a list of strings to a list of vocabulary indeces.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The original word has been split into smaller subwords and characters. \n",
    "The two hash signs preceding some of these subwords are just our tokenizer’s way to \n",
    "denote that this subword or character is part of a larger word and preceded by another subword. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "After breaking the text into tokens, \n",
    "we then have to convert the sentence from a list of strings to a list of vocabulary indeces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” \\nwe must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). \\nFor our purposes, single-sentence inputs only require a series of 1s, \\nso we will create a vector of 1s for each token in our input sentence.\\n\\nIf you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” \n",
    "we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). \n",
    "For our purposes, single-sentence inputs only require a series of 1s, \n",
    "so we will create a vector of 1s for each token in our input sentence.\n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nencoded_layers object has four dimensions, in the following order:\\n    1. The layer number (12 layers)\\n    2. The batch number (1 sentence)\\n    3. The word / token number (22 tokens in our sentence)\\n    4. The hidden unit / feature number (768 features)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "\"\"\"\n",
    "encoded_layers object has four dimensions, in the following order:\n",
    "    1. The layer number (12 layers)\n",
    "    2. The batch number (1 sentence)\n",
    "    3. The word / token number (22 tokens in our sentence)\n",
    "    4. The hidden unit / feature number (768 features)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1, 22, 768])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers.\n",
    "# We use `stack` here to create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 22, 768])\n"
     ]
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Studies for NER have shown that concatenation of the last four layers \n",
    "produced the best results on this specific task.\n",
    "Have a look @ http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png\n",
    "\"\"\"\n",
    "# Creating the word vectors by summing together the last four layers.\n",
    "\n",
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "# For each token in the sentence..\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embedding for 'money':  tensor([ 1.2090e+00, -4.5478e+00, -1.8455e+00,  1.7785e+00,  5.5919e+00,\n",
      "         3.0103e+00, -3.6344e+00, -1.8779e+00, -2.0583e+00, -5.6114e-01,\n",
      "        -5.0866e-01, -3.5573e-02, -1.6982e+00,  3.0808e+00, -5.0677e+00,\n",
      "         9.8939e-01,  3.0563e+00,  3.3990e+00, -5.1202e-01,  2.2999e+00,\n",
      "        -4.7083e+00,  2.1254e-01,  3.1129e+00,  9.1927e-01,  1.8552e+00,\n",
      "        -1.7869e+00, -6.0480e-02,  2.2700e+00, -1.9204e-01,  1.2888e+00,\n",
      "         6.2763e+00,  9.4952e-02,  2.3813e+00,  4.8176e+00,  1.5172e+00,\n",
      "        -1.0869e+00, -3.6432e+00,  1.4986e+00, -5.6087e-01,  1.1351e+00,\n",
      "        -2.6811e-01,  5.8343e-01, -4.2257e+00,  3.7800e+00, -1.6967e-01,\n",
      "         7.2276e-01,  1.3434e+00,  1.0216e+00, -2.5422e+00,  9.3772e-01,\n",
      "        -1.8737e+00,  1.2663e+00, -8.9085e+00, -2.7672e+00,  8.1881e-01,\n",
      "         3.2483e+00, -3.2627e+00, -3.2115e+00,  1.8310e+00, -4.6896e-02,\n",
      "         4.1646e+00, -1.1390e+00,  2.1328e+00, -4.7024e-01, -3.5338e+00,\n",
      "         5.4664e-01,  1.0444e+00,  7.3674e-01, -1.1719e+00,  4.0145e+00,\n",
      "        -9.1610e-01,  1.3602e+00, -1.8375e+00, -5.8753e+00,  1.1199e+00,\n",
      "         3.8553e-01, -2.6666e+00, -7.9877e-02, -1.4671e+00,  1.9823e+00,\n",
      "        -5.0476e+00,  3.1116e+00, -4.0083e-02,  8.9367e-01,  2.5822e+00,\n",
      "         4.6413e-01, -1.7131e+00,  1.9586e-01,  3.3120e-01,  1.7117e+00,\n",
      "         1.6354e+00,  1.6458e+00,  5.4084e-01,  9.9633e-01, -2.3511e+00,\n",
      "         1.0921e-01, -4.6126e-01,  2.4489e+00, -2.7889e+00,  2.4873e+00,\n",
      "         4.5055e+00, -2.5600e+00, -6.9938e-01, -4.4043e-01, -6.7553e-01,\n",
      "         2.3535e+00,  1.2837e+00, -1.0640e-01, -1.3001e-01, -1.6295e-01,\n",
      "        -5.1879e-01, -2.0264e+00,  5.0051e-01, -6.2098e+00, -5.1121e-01,\n",
      "        -3.5299e+00,  2.6677e+00,  5.8253e-02, -4.4715e+00,  1.1863e-02,\n",
      "         2.9056e+00,  5.5065e-01, -1.3492e+00,  5.1863e-01,  8.5071e-01,\n",
      "        -3.3457e-01,  2.3638e+00,  1.6977e+00, -2.3155e+00, -2.1220e+00,\n",
      "        -3.5477e+00,  3.9967e-02,  2.4101e-01,  1.2528e+00,  1.8408e+00,\n",
      "         3.2558e-01, -4.1510e+00, -8.1586e-01, -4.2108e+00,  1.5264e+00,\n",
      "         1.6683e+00,  5.1689e+00,  1.2570e+00,  6.1672e+00, -9.8552e-01,\n",
      "         3.7214e+00,  6.5520e-01, -1.2097e+00, -1.8708e+00,  2.4851e+00,\n",
      "        -9.9555e-01,  4.0756e-01,  6.0613e-02,  2.8983e+00,  3.4257e+00,\n",
      "        -1.7210e-01, -7.6196e-01,  7.0975e-01, -2.8439e+00, -2.6285e+00,\n",
      "         1.5398e+00,  1.5869e+00,  2.4455e-02, -7.4638e-02, -2.8457e+00,\n",
      "         2.5742e+00,  4.8982e+00,  4.8168e+00, -3.2542e+00, -2.9860e+00,\n",
      "        -2.3616e+00,  1.6760e+00,  8.3458e-02,  2.7632e-01,  2.5825e+00,\n",
      "         2.4198e+00,  1.4351e-01, -2.0125e+00,  2.7785e-01, -1.5645e+00,\n",
      "        -2.8309e-01,  2.6403e+00,  2.9292e+00,  3.4879e+00,  1.4947e+00,\n",
      "         2.0876e-01,  1.6551e+00, -2.2725e+00,  2.0085e+00, -2.8114e+00,\n",
      "        -1.3612e+00, -6.1253e-01, -2.5208e+00,  2.2723e+00, -3.0739e-01,\n",
      "         1.1831e-01,  3.1180e+00, -3.7280e+00,  4.9035e-01, -2.9522e+00,\n",
      "        -3.1424e+00,  2.2156e-01, -1.5891e+00,  9.9755e-02, -1.5480e+00,\n",
      "         1.4531e-01, -9.9641e-01,  2.5155e+00, -2.7663e+00, -2.6628e+00,\n",
      "        -1.3378e+00,  1.7510e+00,  1.1453e+00,  8.9307e-01, -3.8786e+00,\n",
      "        -1.0959e+00, -1.2859e+00, -2.8105e+00,  4.1648e-01,  2.9446e+00,\n",
      "        -1.7166e+00,  2.4148e+00,  4.4602e+00,  1.1145e+00, -5.3998e+00,\n",
      "         3.4199e+00,  8.6758e-01, -1.5220e+00,  2.1146e+00,  2.1485e+00,\n",
      "         1.9629e+00, -2.0750e+00,  1.8731e+00, -6.3194e-01,  1.2229e+00,\n",
      "        -7.3416e-01, -2.0066e+00,  4.4138e-01,  3.1506e+00, -1.2654e+00,\n",
      "         3.8838e+00, -3.7670e+00,  1.3458e+00, -2.3523e+00,  3.8422e-01,\n",
      "         1.7819e+00, -9.1179e-02, -1.1353e+00, -1.7529e+00, -7.0320e-01,\n",
      "         7.8438e-01, -2.1091e+00, -2.4478e+00, -2.0359e+00,  5.3096e+00,\n",
      "         1.0351e+00, -5.1175e-01,  6.4718e-01, -2.3630e+00,  1.6074e+00,\n",
      "         6.5363e+00,  7.3423e-01, -3.3866e-01, -1.2227e+00,  1.9615e+00,\n",
      "        -2.7968e+00, -4.8661e-01, -2.9563e-01, -9.7868e-01,  3.7954e+00,\n",
      "         2.6320e+00, -3.2154e-01, -1.9965e+00,  1.6369e+00,  7.3200e-01,\n",
      "        -5.2064e+00, -2.1222e+00,  7.1997e-01, -2.6093e-01,  1.5400e+00,\n",
      "         1.1042e+00,  1.2275e-01, -2.0261e+00, -2.2229e+00, -6.3127e-01,\n",
      "        -1.5222e+00,  3.2215e+00,  6.0392e+00, -2.7216e-01,  6.5049e-01,\n",
      "        -2.4685e-01,  1.1382e+00, -3.0401e+00,  8.5742e-01,  1.2631e-01,\n",
      "         2.9006e+00, -4.5829e-01,  4.3464e+00, -4.2481e-01,  1.5142e+00,\n",
      "         4.5242e+00,  6.5001e-01, -1.8593e+00,  9.4850e-01, -2.2785e+00,\n",
      "        -2.0727e+00,  3.7236e+00, -3.9504e-01, -2.7752e+01,  5.3547e+00,\n",
      "         1.9866e+00, -2.1816e+00, -2.0573e+00,  5.7506e-01,  1.9794e+00,\n",
      "        -2.3684e+00, -1.8277e+00, -3.6621e+00, -4.1718e+00, -3.2440e+00,\n",
      "         4.5994e-01,  2.7213e-01, -6.2494e-02, -1.0202e+00, -4.6511e-01,\n",
      "        -4.4928e-01, -2.6495e-01,  1.0652e+00, -1.8194e+00,  8.5385e-01,\n",
      "        -1.1655e+00,  2.5689e+00,  3.1968e+00,  3.8009e+00,  9.0902e-01,\n",
      "        -6.8012e-01,  7.6591e-01,  1.5362e+00, -4.0218e+00, -1.8546e+00,\n",
      "        -1.6373e-01, -5.2158e+00, -1.8963e-01, -1.9298e+00, -1.1800e+00,\n",
      "        -2.4144e+00,  2.9380e+00, -1.1831e+00, -3.3491e-01, -1.5047e+00,\n",
      "        -1.1636e-02, -3.1074e+00,  3.8471e+00,  1.4160e+00, -4.5148e+00,\n",
      "         1.3792e+00, -2.9682e+00,  1.7404e+00,  1.3565e+00, -2.0739e+00,\n",
      "        -2.8225e+00,  9.8936e-01,  6.2070e-01,  5.9624e+00,  1.1235e+00,\n",
      "         7.6023e-01,  1.9697e+00,  1.8811e+00,  2.3949e-01, -2.5391e-01,\n",
      "        -5.0402e-02,  2.7804e+00, -4.9482e-01, -1.7242e+00, -3.8207e+00,\n",
      "         3.9229e+00,  2.2822e+00, -1.5835e+00, -1.9828e+00,  8.2009e-01,\n",
      "        -4.9489e-01, -9.7762e+00, -1.2360e-01, -5.1857e-01,  4.2703e+00,\n",
      "         1.0247e+00, -5.1070e+00, -7.9629e-01,  9.5288e-01,  7.1183e-01,\n",
      "         2.7632e+00, -1.9050e+00,  1.6387e+00,  1.7282e+00, -3.9303e+00,\n",
      "         5.3752e-01, -4.9452e+00, -2.4620e+00,  7.9795e-01,  1.2652e+00,\n",
      "         3.1066e+00, -7.7309e-01,  1.0577e+00, -6.0956e-01, -2.1460e+00,\n",
      "         1.0376e+00, -8.5600e-01, -3.9756e-01, -5.9791e-01, -1.4918e+00,\n",
      "         2.7925e-01, -2.2748e+00, -2.8386e+00,  5.3902e-01, -4.5588e-01,\n",
      "         1.8522e+00, -2.8433e+00, -8.3048e-02,  2.1444e+00, -2.3933e+00,\n",
      "         2.7663e+00,  1.2176e+00,  3.3632e-01,  7.5039e-01,  7.1833e-01,\n",
      "        -9.1428e-01,  2.7245e-01, -3.1183e+00, -1.2246e-02, -1.0611e+00,\n",
      "        -4.2665e+00, -2.7377e+00, -9.6142e-01,  1.4491e+00,  3.2659e+00,\n",
      "        -2.0827e+00, -5.9135e-01, -2.2507e+00, -5.8647e-01, -2.2516e+00,\n",
      "        -1.9999e+00,  1.2357e+00,  1.9567e+00,  3.1313e-01,  4.5147e+00,\n",
      "        -4.8148e+00,  1.0014e+00, -1.4689e+00,  1.0126e+00, -1.9853e+00,\n",
      "         2.0696e+00, -8.2391e-01, -1.4862e+00, -4.0143e-01, -1.1582e+00,\n",
      "        -9.1512e-02, -3.3745e-01,  7.0527e-01, -3.9881e+00, -3.5571e+00,\n",
      "         9.6303e-01, -3.6764e-01,  8.3030e-01,  3.8316e+00,  2.3067e+00,\n",
      "        -1.3247e-01, -2.1523e+00, -4.9277e+00, -1.5936e+00,  1.7735e+00,\n",
      "        -1.9360e+00,  1.9512e-01, -5.1219e+00, -1.8137e-01, -2.5650e+00,\n",
      "        -3.5745e+00,  1.3700e+00,  4.9667e+00,  4.3008e+00, -2.9572e+00,\n",
      "         2.2824e+00,  3.4156e+00, -9.3630e-01,  2.4994e+00,  3.3322e-01,\n",
      "        -6.1519e+00, -3.6257e+00,  1.1870e+00, -5.3563e-01,  1.8637e+00,\n",
      "         1.9078e+00, -2.1869e+00, -1.6724e+00,  1.2708e+00,  2.1714e+00,\n",
      "        -9.8668e-01, -1.0644e+00,  7.1455e-01,  1.4410e+00,  2.0220e+00,\n",
      "         1.5397e+00,  2.6256e+00,  1.7033e+00, -2.8682e+00, -3.1925e-01,\n",
      "         1.1706e+00, -1.4422e+00, -9.1016e-01,  1.0762e-01, -9.2249e-01,\n",
      "         3.2494e-01, -1.3949e+00,  1.7865e+00, -1.2097e+00,  2.6524e+00,\n",
      "         6.9728e-01, -4.5819e-01,  2.8796e+00, -7.2564e-01,  1.8577e+00,\n",
      "        -5.6164e+00, -1.3515e+00,  8.5062e-01, -2.6732e+00,  6.0142e-01,\n",
      "        -1.7736e+00, -2.9098e+00, -1.5704e+00,  4.8419e-01,  2.8794e+00,\n",
      "         1.7576e+00,  3.1220e+00, -1.0475e+00, -1.8733e+00, -1.5186e-01,\n",
      "        -2.2451e+00,  2.5596e+00,  4.2046e-01, -4.1906e-01, -6.3998e+00,\n",
      "         4.2290e-01, -1.6190e+00,  2.0335e+00,  3.3809e-02, -1.0985e-01,\n",
      "        -4.3604e+00, -1.9243e+00,  5.9539e-01,  6.3129e-01, -3.2065e-01,\n",
      "         2.4872e+00, -7.1209e-01,  8.6209e-01,  5.1329e-01,  1.1474e+00,\n",
      "         1.7057e+00,  5.8018e-01,  3.1804e-01,  1.8989e+00,  7.9007e-01,\n",
      "         1.8469e+00,  1.7373e-01, -2.5261e+00,  4.2655e+00,  1.4779e+00,\n",
      "         2.0935e+00, -8.2874e-01,  2.5371e+00,  2.7655e-01, -5.9233e+00,\n",
      "        -7.6914e-01, -8.7511e-01,  1.7877e+00, -3.0150e-01, -2.9336e+00,\n",
      "         3.7115e+00,  2.3916e+00, -2.5170e+00,  3.6486e+00, -4.8587e+00,\n",
      "         5.4765e-01, -1.8268e+00, -3.0725e+00, -1.3702e+00, -8.0759e-01,\n",
      "        -7.7915e+00,  8.3965e-01, -7.8647e-01, -2.2558e+00,  2.2005e+00,\n",
      "        -1.2042e+00, -2.3145e-01, -3.9241e-01,  1.5780e+00, -2.8435e+00,\n",
      "        -8.9876e-01, -1.7758e-01, -2.8986e+00, -6.2282e-01,  3.1926e+00,\n",
      "         1.4546e+00, -3.0175e+00, -4.5731e+00,  1.7884e+00,  3.3270e+00,\n",
      "        -7.1543e-01,  9.7997e-01,  1.4671e+00,  2.9951e+00, -2.3227e+00,\n",
      "        -1.1931e-01,  1.9768e+00, -2.6490e+00,  4.4418e+00,  7.1293e-01,\n",
      "         1.3394e-01, -3.1775e-01, -4.7882e+00, -4.3046e+00, -4.2421e+00,\n",
      "         1.1187e+00, -2.7397e+00, -6.0922e-01,  2.7033e+00, -1.4191e-01,\n",
      "        -5.7916e-01, -2.5103e+00,  8.3894e-01, -1.2283e+00,  2.0046e+00,\n",
      "        -1.5189e+00,  1.8639e+00, -3.2994e+00,  3.1380e+00,  5.7080e+00,\n",
      "        -2.3018e+00,  3.9277e-01, -7.5030e-01, -2.6826e+00,  5.7212e+00,\n",
      "        -4.1941e-01,  2.9277e-01,  2.8005e-01,  5.5061e-01, -2.7496e+00,\n",
      "        -5.4515e+00,  4.5399e+00,  7.7185e-01, -1.8174e+00, -5.6385e-01,\n",
      "        -1.4944e+00,  6.2573e+00,  7.3741e-01,  2.3495e-01, -1.2785e+00,\n",
      "        -2.5013e+00,  2.0756e+00,  3.5277e+00,  4.3492e-01,  2.3703e-01,\n",
      "         7.7962e-01, -5.9228e+00,  2.9368e+00,  2.8091e-01,  1.3602e+00,\n",
      "         7.0774e-01, -2.8943e+00,  3.5828e-01,  3.8513e-01, -2.5296e-02,\n",
      "         2.6891e+00, -1.3036e+00, -1.3514e+00,  8.6428e-02, -2.7681e+00,\n",
      "        -1.9854e+00, -4.5978e-01, -2.8402e-01,  1.8298e-01,  1.9455e+00,\n",
      "        -2.5940e+00, -8.9590e-01,  1.3002e+00,  7.8552e-01,  9.7392e-01,\n",
      "         2.9289e+00,  1.8337e+00, -5.5413e-01,  1.5361e+00, -2.1719e+00,\n",
      "        -6.1608e-01, -1.7641e+00, -6.6320e-01, -2.6765e-01, -4.8419e-01,\n",
      "         7.7113e-01, -1.1754e+00, -4.3043e-01,  1.9664e+00,  6.6126e-01,\n",
      "        -6.9275e-01, -1.0316e-01, -1.6362e+00,  7.8134e-01,  1.7326e-01,\n",
      "         1.8157e+00,  1.9925e+00,  1.1012e+00, -2.5844e+00, -1.3334e+00,\n",
      "         9.5469e-01, -5.7265e-01, -3.8507e+00, -9.4351e-01,  1.2411e-01,\n",
      "        -1.6430e-01, -4.8340e+00,  9.9835e-01,  1.1292e+00, -9.8605e-02,\n",
      "        -3.2825e-01,  5.0703e-02,  1.1778e-01, -2.4850e+00,  3.9862e-01,\n",
      "        -9.4341e-01, -1.5798e+00, -1.3437e+00,  4.6652e-01,  1.5205e+00,\n",
      "         2.3125e-01,  3.5742e+00, -1.4512e+00,  1.7765e+00,  1.1647e+00,\n",
      "        -1.9618e+00, -1.6947e+00, -8.6528e-01, -2.4613e+00,  2.0236e+00,\n",
      "         1.1573e+00,  1.3431e+00, -9.8140e-02, -3.4002e+00, -2.6948e-01,\n",
      "        -2.3720e-01, -9.2647e-01,  3.9892e-01,  3.5015e+00,  4.6014e-01,\n",
      "         2.9543e-01,  1.4319e+00, -1.6974e+00,  3.7497e+00, -2.0742e+00,\n",
      "        -1.7250e+00, -3.6750e+00,  5.2366e+00, -1.8422e+00, -2.3475e+00,\n",
      "         1.7806e+00,  8.1584e-02,  1.5951e+00, -7.3080e-01,  8.3426e-01,\n",
      "        -1.4083e+00,  3.7154e-01,  1.7883e+00])\n"
     ]
    }
   ],
   "source": [
    "print(\"word embedding for 'money': \", str(token_vecs_sum[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.9456751942634583\n",
      "Vector similarity for *different* meanings:  0.6797332763671875\n"
     ]
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = cos(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = cos(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  {}' .format(same_bank))\n",
    "print('Vector similarity for *different* meanings:  {}' .format(diff_bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
