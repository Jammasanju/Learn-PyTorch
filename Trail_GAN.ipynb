{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "SEED = 25\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_WordEmbeddingsPipeline import embeddingsPipeline\n",
    "embedding_length = 768\n",
    "zero_embedding = [0 for i in range(embedding_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     tokenized_sents\n",
      "0  It never once occurred to me that the fumbling...\n",
      "1  Finding nothing else not even gold the Superin...\n",
      "2  Herbert West needed fresh bodies because his l...\n",
      "3  The farm like grounds extended back very deepl...\n",
      "4  His facial aspect too was remarkable for its m...\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('hpl.csv')\n",
    "data = data[:100]\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100\n",
    "\n",
    "emb = []\n",
    "for i in data['tokenized_sents']:\n",
    "    e = embeddingsPipeline(i)\n",
    "    while(len(e) < max_words):\n",
    "        e.append(zero_embedding)\n",
    "    e = e[:max_words]\n",
    "    emb.append(e)\n",
    "    \n",
    "data['embeddings'] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 2) (10, 2) (10, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, remaining_data = train_test_split(data, test_size=0.2, random_state=SEED)\n",
    "test_data, valid_data = train_test_split(remaining_data, test_size=0.5, random_state=SEED)\n",
    "\n",
    "print(train_data.shape, test_data.shape, valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # initializing weights\n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "         \n",
    "    def forward(self, input):\n",
    "        input = self.dropout(input)\n",
    "        encoded_input, hidden = self.lstm(input)\n",
    "        encoded_input = self.relu(encoded_input)\n",
    "        return encoded_input\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, output_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # initializing weights\n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "       \n",
    "    def forward(self, encoded_input):\n",
    "        decoded_output, hidden = self.lstm(encoded_input)\n",
    "        return decoded_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, input):\n",
    "        encoded_input = self.encoder(input)\n",
    "        decoded_output = self.decoder(encoded_input)\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, max_words):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, 1, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(max_words, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, hidden = self.lstm(input)\n",
    "        output = torch.flatten(output)\n",
    "        output = self.fc1(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = embedding_length\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = embedding_length\n",
    "\n",
    "enc = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS)\n",
    "dec = Decoder(HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n",
    "gen = Generator(enc, dec)\n",
    "\n",
    "dis = Discriminator(INPUT_SIZE, 1, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(data, gen, dis, epochs, max_words, embedding_length):\n",
    "    gen.train()\n",
    "    dis.train()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gen.to(device)\n",
    "    dis.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    d_optimizer = torch.optim.Adam(dis.parameters(), lr=0.0002)\n",
    "    g_optimizer = torch.optim.Adam(gen.parameters(), lr=0.0002)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        real_label = torch.ones(1).to(device)\n",
    "        fake_label = torch.zeros(1).to(device)\n",
    "        \n",
    "    # Training Discriminator\n",
    "        total_discriminator_loss = 0\n",
    "        for text in data:            \n",
    "            text = torch.Tensor(text)\n",
    "            text = text.to(device)\n",
    "            text = text.view(max_words, 1, embedding_length)\n",
    "            dis_out = dis(text)\n",
    "            d_loss_real = criterion(dis_out, real_label)\n",
    "            \n",
    "            fake_text = gen(text)\n",
    "            fake_text = fake_text.view(max_words, 1, embedding_length)\n",
    "            dis_out = dis(fake_text)\n",
    "            d_loss_fake = criterion(dis_out, fake_label)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            total_discriminator_loss += d_loss.item()\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "        \n",
    "    # Training Discriminator\n",
    "        total_generator_loss = 0\n",
    "        for text in data:\n",
    "            text = torch.Tensor(text)\n",
    "            text = text.to(device)\n",
    "            text = text.view(max_words, 1, embedding_length)\n",
    "            \n",
    "            generated_text = gen(text)\n",
    "            dis_out = dis(generated_text)\n",
    "            \n",
    "            g_loss = criterion(dis_out, real_label)\n",
    "            total_generator_loss += g_loss.item()\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "        \n",
    "        finish_time = time.time()\n",
    "        \n",
    "        avg_d_loss = total_discriminator_loss/len(data)\n",
    "        avg_g_loss = total_generator_loss/len(data)\n",
    "        time_taken = finish_time-start_time\n",
    "        \n",
    "        print(\"Epoch[{:02}/{}]: average_d_Loss: {:.4f}, average_g_Loss: {:.4f}, Time for epoch: {:.4f}\" \n",
    "             .format(epoch+1, epochs, avg_d_loss, avg_g_loss, time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[01/25]: average_d_Loss: 1.3922, average_g_Loss: 0.6519, Time for epoch: 3.3999\n",
      "Epoch[02/25]: average_d_Loss: 1.1759, average_g_Loss: 0.7216, Time for epoch: 3.1256\n",
      "Epoch[03/25]: average_d_Loss: 1.3395, average_g_Loss: 0.5841, Time for epoch: 3.1246\n",
      "Epoch[04/25]: average_d_Loss: 1.3160, average_g_Loss: 0.7826, Time for epoch: 3.1276\n",
      "Epoch[05/25]: average_d_Loss: 1.0386, average_g_Loss: 1.0207, Time for epoch: 3.1376\n",
      "Epoch[06/25]: average_d_Loss: 0.8382, average_g_Loss: 1.0028, Time for epoch: 3.1675\n",
      "Epoch[07/25]: average_d_Loss: 1.0375, average_g_Loss: 0.7140, Time for epoch: 3.1745\n",
      "Epoch[08/25]: average_d_Loss: 0.9628, average_g_Loss: 0.7318, Time for epoch: 3.1296\n",
      "Epoch[09/25]: average_d_Loss: 0.8924, average_g_Loss: 0.7491, Time for epoch: 3.1486\n",
      "Epoch[10/25]: average_d_Loss: 0.8298, average_g_Loss: 0.7661, Time for epoch: 3.3321\n",
      "Epoch[11/25]: average_d_Loss: 0.7756, average_g_Loss: 0.7827, Time for epoch: 3.4787\n",
      "Epoch[12/25]: average_d_Loss: 0.7311, average_g_Loss: 0.7990, Time for epoch: 3.3371\n",
      "Epoch[13/25]: average_d_Loss: 0.6942, average_g_Loss: 0.8147, Time for epoch: 3.3131\n",
      "Epoch[14/25]: average_d_Loss: 0.6633, average_g_Loss: 0.8301, Time for epoch: 3.2164\n",
      "Epoch[15/25]: average_d_Loss: 0.6370, average_g_Loss: 0.8452, Time for epoch: 3.2164\n",
      "Epoch[16/25]: average_d_Loss: 0.6144, average_g_Loss: 0.8599, Time for epoch: 3.2433\n",
      "Epoch[17/25]: average_d_Loss: 0.5947, average_g_Loss: 0.8743, Time for epoch: 3.2493\n",
      "Epoch[18/25]: average_d_Loss: 0.5772, average_g_Loss: 0.8884, Time for epoch: 3.2693\n",
      "Epoch[19/25]: average_d_Loss: 0.5614, average_g_Loss: 0.9024, Time for epoch: 3.2723\n",
      "Epoch[20/25]: average_d_Loss: 0.5472, average_g_Loss: 0.9161, Time for epoch: 3.2762\n",
      "Epoch[21/25]: average_d_Loss: 0.5341, average_g_Loss: 0.9297, Time for epoch: 3.3122\n",
      "Epoch[22/25]: average_d_Loss: 0.5220, average_g_Loss: 0.9431, Time for epoch: 3.3131\n",
      "Epoch[23/25]: average_d_Loss: 0.5107, average_g_Loss: 0.9564, Time for epoch: 3.3301\n",
      "Epoch[24/25]: average_d_Loss: 0.5001, average_g_Loss: 0.9696, Time for epoch: 3.3311\n",
      "Epoch[25/25]: average_d_Loss: 0.4901, average_g_Loss: 0.9827, Time for epoch: 3.3281\n"
     ]
    }
   ],
   "source": [
    "trainGAN(data['embeddings'], gen, dis, 25, max_words, embedding_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
